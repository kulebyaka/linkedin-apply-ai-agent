# Implementation Plan & Status

## Functional requirements:

Every hour, based on a set of filters (in the future, filters will be built based on the user's verbal request), it makes a request and the Linkedin API returns a set of positions. LLM evaluates each position to determine whether it actually matches the request (the description may contain information that it is not actually a remote position, etc.). Positions that pass the filter are matched with the user's data set (very comprehensive CV in JSON format). Based on the position description and CV a new JSON that represents the CV with recomposed relevant information to the position information is generated by llm. A PDF is created from the JSON. The agent goes to LinkedIn (not sure how, via browser mcp or using some skill) and attempts to apply for the position. If unsuccessful, it sends a notification to the user (webhook) On every step it should be able to leverage the HITL if there is some uncertainty about successfulness of the current step. Also we need HITL on the very last step to review the modified CV and the position before submitting. Do it it batches or in some tinder-like UI so user sees the job position + new CV and could approve (swipe right) or decline (swipe left and write a little note, why it is declined after swiping) or ask for try again (swipe down, also add the note what to do differently)

## Non-Functional requirements:

- Self-hosted on a VPS
- Supports at least 3 different LLM providers (e.g., OpenAI, DeepSeek, Grok, etc.) and can switch between them easily
- Modular design to allow easy updates and maintenance
- Prefer Playwright for browser automation

## Implementation Status

| Component | Status | Notes |
|-----------|--------|-------|
| **Architecture / Workflow** | ðŸŸ¡ Partial | LangGraph skeleton defined in `src/agents/workflow.py`. |
| **LLM Provider Layer** | âœ… Complete | Implemented in `src/llm/provider.py`. |
| **Node 1: Fetch Jobs** | ðŸ”´ Pending | `src/services/job_fetcher.py` is a skeleton. Needs LinkedIn scraping/API logic. |
| **Node 2: Filter Job** | ðŸ”´ Pending | `src/services/job_filter.py` skeleton exists. Prompts defined but LLM call generic. |
| **Node 3: Compose Tailored CV**| âœ… Complete | `src/services/cv_composer.py` fully implemented with logic to tailor all sections. |
| **Node 4: Generate PDF** | âœ… Complete | `src/services/pdf_generator.py` fully implemented & tested. Uses WeasyPrint + Jinja2. |
| **Node 5: Human Review** | ðŸ”´ Pending | Logic in workflow exists, but UI/API interaction needs implementation. |
| **Node 6: Apply on LinkedIn** | ðŸ”´ Pending | `src/services/browser_automation.py` is a skeleton. Needs Playwright implementation. |
| **Node 7: Notification** | ðŸ”´ Pending | `src/services/notification.py` is a skeleton. |

## Architecture: Job Application Automation Agent

Orchestrated Workflow with LangGraph (Modular Pipeline)

Overview: This option keeps a Python-centric solution but introduces a structured workflow/orchestration framework â€“ specifically LangGraph (a 2025 successor to LangChain that uses explicit graph/state machine definitions for LLM-driven tasks). Instead of one big script, we define the pipeline as a directed graph of tasks: nodes for each step (job search, filtering, CV crafting, application, etc.) and edges to control flow (including conditional branches and human approval loops). All components can still run on one VPS (perhaps as separate threads or async tasks), but logically they are modular. This design provides more control and observability than a basic script, and it naturally integrates the HITL steps and multi-model support through the LangGraph framework features.

Design & Components:

Workflow Definition: Using LangGraph, we model each stage as a node in an agent workflow graph

For example:

Node 1: Fetch Jobs [PENDING] â€“ an extraction node that gets job postings. It uses a "Clean & Extract" strategy:
    1. Fetch raw HTML of the job posting (via Playwright or HTTP request).
    2. Convert HTML to clean Markdown (stripping boilerplate, keeping structure).
    3. Send the full Markdown to a cost-effective High-Context LLM (e.g., GPT-4o-mini or DeepSeek) to directly extract structured data (Role, Company, Description, Requirements) using a strict JSON schema.
    This modern approach avoids the complexity of vector databases (RAG) and is more robust than CSS selectors alongside being cost-effective with current models.

Node 2: Filter Job (LLM) [PENDING] â€“ an LLM task node that takes the structured job description and user filter criteria, and returns a boolean or score. If the LLM (e.g., GPT-4o-mini or DeepSeek model) says the job is unsuitable, a branch directs the flow to skip applying for that job.

Node 3: Compose Tailored CV [COMPLETED] â€“ an LLM node that takes the userâ€™s master CV data and the job description, and outputs structured data (JSON) for a tailored resume. LangGraph supports maintaining long-term memory or context, but here we mainly need a single-step prompt. The output can be validated by a Python function sub-node (to ensure JSON format).
    - *Implementation Status:* Fully implemented in `src/services/cv_composer.py`. Handles summarization, experience tailoring, and strict schema validation.

Node 4: Generate PDF [COMPLETED] â€“ a tool node that converts the JSON to PDF (calls a Python function using a library like WeasyPrint).
    - *Implementation Status:* Fully implemented in `src/services/pdf_generator.py`. Includes unit tests and modern Jinja2 templates.

Node 5: Human Review [PENDING] â€“ this is a special Human-in-the-Loop node where execution pauses until a human approves. One of LangGraphâ€™s features is the ability to insert moderation or approval steps so that an agentâ€™s action can require external confirmation
We can configure this node to present the job info and PDF to the user. In practice, LangGraph might expose a UI or at least an API where our front-end can fetch pending approvals. Weâ€™d likely implement a small web frontend that reads from a LangGraph monitoring API or database to show the pending application and has buttons to respond (approve/decline/retry with note). The userâ€™s decision can be fed back into the workflow (LangGraph might treat it as an input message that triggers the next path).

- If approved, flow continues.

- If declined, flow ends for that job (and maybe logs the reason).

- If retry with changes, we could loop back to Node 3 (CV composition) with the userâ€™s feedback appended to the prompt (this is where LangGraphâ€™s dynamic graph capabilities shine â€“ we can loop or adjust and call the LLM again).

Node 6: Apply on LinkedIn [PENDING] â€“ a final action node that uses an automation tool to submit the application. This could be implemented via a tool integration (LangGraph can likely call Python functions or shell commands; weâ€™d have a function for the browser automation to log in and apply).

Node 7: Notification [PENDING] â€“ an error-handling path that sends a notification if any previous node failed. LangGraph allows adding handlers for exceptions or timeouts; these can trigger a function to send a webhook or email alert to the user.

Execution: The LangGraph workflow can be triggered on a schedule. We might still use an external scheduler (cron) to start it hourly, or use LangGraphâ€™s own scheduling capabilities if it has any. Each run processes available new jobs. Because the workflow is defined at a high level, LangGraph can potentially run multiple jobs in parallel (for example, multiple instances of the graph for different postings). However, caution is needed with concurrency if using the same browser session â€“ likely we process jobs sequentially or with limited concurrency.

Multi-LLM Support: LangGraph is model-agnostic; we can configure different LLM nodes to use different providers. For example, perhaps use a cheaper model (like DeepSeek or Grok) for the filtering step and a more powerful one (GPT-4) for generating the tailored CV content. LangGraphâ€™s Strategic LLM Selection guide 
docs.crewai.com
 suggests it supports choosing models per task. Each LLM node can be configured with an API key and model name, and switching providers is as easy as changing that configuration (no code changes to the workflow logic). This satisfies the requirement to support at least three providers â€“ we just plug their APIs into the LangGraph LLM interface. (If needed, fallback logic can be included: e.g., if one call fails or returns insufficient info, try another model. This can be done via conditional branches or error handlers in the graph.)

HITL Integration: As mentioned, one big advantage of LangGraph is first-class support for human approvals. It allows the agent to pause and wait for a human decision mid-flow. Implementing the UI for this might require using LangChainâ€™s or LangGraphâ€™s UI modules or building a custom small app. Since this is self-hosted, we can run a minimal web server (Flask/FastAPI) that the user can connect to. LangGraph might store pending actions in a memory or database which our UI can query. Once the user clicks approve/decline, the UI calls a LangGraph API to resume the workflow along the chosen path. This mechanism ensures no application is submitted without approval. It also allows incorporating user feedback (notes on why declined or what to change for retry). Those can be fed back into the LLM prompt for a second iteration.